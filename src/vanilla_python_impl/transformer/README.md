TOBEDONE: 

> we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30].

https://stackoverflow.com/questions/70308466/why-are-weight-matrices-shared-between-embedding-layers-in-attention-is-all-you
